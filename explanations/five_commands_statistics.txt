1 Why we need to do statistics in the first place

When analysing data we hope to learn something about the state of the world. We don’t care about the dataset specifically, but rather about the relationships that we hope to learn from it. When showing two designs to two groups of users we want to learn which design is preferred by all visitors of the website, instead of just the preference of the people who ended up in our AB test. Probably without completely realising it, we are constantly making inferences from a sample to a population.  

The biggest foe of inference from data to population, is variance in the thing we measure that is caused by other sources than the one we are interested in. If we want to know if the new design is better than the old design, we measure the click-through-rate of users that were shown one of the designs. If design would be the only thing that determines if a user clicked or not, it would very easy to confirm which design is better. The click-through-rate of one design would be 0 and of the other would be 1. Of course, design is not the only thing determining the click-through. A motivated user would click irrespective of the quality of the design, and someone who is just casually browsing wouldn’t click even on the best possible design. User motivation therefore is a factor that gets in the way of measuring which design is best. 

The variance in the thing we measure that is not caused by the variable we are interested in goes by many names, such as error variance, randomness and noise. Whatever we call it, it causes us to be unsure of the effect of the variable under study. We see that in the sample the new design performs better, but is this really caused by the quality of the design or did we happen to have more motivated people in the right group by chance? This is exactly the type of question for which we invoke statistics. It quantifies the uncertainty we have on our estimate, thereby providing a probability for all the possible values of the thing we are interested in. If the click-through-rate in the sample happens to be 0.43, statistics will tell us the probability that the true click-through-rate for example is 0.42 (which will be high) or is 0.17 (which will be very low). 

 

2 Why we need a substantial sample 

Uncertainty about the population value of the thing we want to know more about is influenced by two factors. The first one is probably intuitive, namely the number of cases we have in our sample. If we ask just five people how they feel about the new feature we recently introduced, and a majority responded negatively, it is very well possible that we happen to have asked a bunch of sourpusses who would have rejected every possible new feature to the website. But if you ask a few hundred people and the majority did not respond positively, we can no longer assume that the cheerfulness of the users that happened to be in the sample interfered with the awesomeness of the new feature. It is pretty safe to say the new feature is not perceived as awesome.   

As we increase the sample size, the sample is guaranteed by the rules of probability to be a more representative set of cases of the population. If in the real world 30% of the people is gloomy and 70% is cheerful, it could be quite possible that we end up with a negative majority in our sample of size five (for those keen on math the probability is 0.163). However, if our sample has five hundred people on it there is no way this would happen (this is actually probability is too small to compute on a 64 bit machine). No matter the mathematics, just keep the following rule in mind:  

As the number of people in our sample increases, the sample is guaranteed to look more and more like the general population. When the sample gets large enough, we can rule out other factors disturb our analysis. 

 

3 Why your sample needs to be representative 

Whether you are aware of it or not, when doing data analysis you generalise the results in your sample to the entire population of people you are interested in. We say “the user has a greater satisfaction with funda when this feature is implemented” instead of “the 714 people who happen to be in our sample have a greater satisfaction with funda”. This is, of course, the aim of doing research in the first place, we want to know what all people think, not just a subset. We discussed earlier that we need a sufficient number of cases in our sample to rule out that the effect is caused by chance. Fortunately, statistical inference formalises the probability that the effect is artificial due to a sample that is too small. 

However, it does not guard us against generalising to the incorrect population. If we want to know what all realtors in the Netherlands think of the new service we are about to introduce and we test feature only with realtors in Amsterdam because they happen to be close to the office, we have a generalisation problem. Realtors in Amsterdam probably serve a very different market and have a different personality than those active in more rural parts of the country. Taking such a convenience sample can create major flaws in our knowledge. 

In the most ideal scenario, every single person in the population has the exact same probability to be included in our sample. This is seldom the case. For instance, running an A/B test for a few weeks selects a limited group of users, even when all the users in the group have an equal probability to be included. The test was only conducted in a certain month of the year, while the housing market was at a high or a low. It is unlikely this limits the generalisation of the results when we are comparing two designs. But what about testing two different prices of a new product?  

As said, a complete random sample out of the entire population is seldom attainable. But the analyst has the responsibility to create as little generalisation bias as possible. As part of the analysis results you should always give full disclosure about how the sample was constructed and if you foresee problems with generalisation. These sample characteristics are intrinsically part of your results and should therefore always be reported alongside the statistical findings. 

 

4 Effect size 

Statistical tests answer the question based on the sample at hand, can we conclude there is an effect in the population? With an effect we usually mean a difference between groups, such as click-through-rates of different designs or number of purchases between two types of campaigns. There are two things that influence the confidence we have that an observed effect in the sample is real, thus not due to chance because of sampling. They are the number of sample cases, as discussed at two, and the size of the effect in the sample. If you find an effect on a large sample, it is less likely this due to chance than when your sample is small. And confidence grows as the effect in the sample gets larger. 

Note that a statistical test, such as the infamous p-value or the Bayesian posteriors we are using, has nothing to say about the actual size of the effect. It only reflects the confidence we have in the effect being real as a mathematical reflection of the sample and effect size. Many researchers stop at the question “is the effect real?” by passing the statistical test or not. However, you should always ask the follow-up question “is the effect large enough to be relevant?”. This question should be answered in light of the business decision that is tied to the outcome of the data analysis. The follow-up actions on the outcome of the analysis usually differ in costs. If the new design has a click-through-rate that is 0.09% higher than the old design, this effect may prove to be statistically real if it is tested on thousands on users. However, if it takes a team four sprints to implement, the benefits in click-through-rate might not outweigh the costs of implementation and we choose to stick to the old design. 

It is always the responsibility of the analyst to communicate the estimated effect size. It does not suffice to claim one is better, but you should always say how much better.  

5 Some practical tips for doing an analysis 

In spite of its halo of objectivity and scientificity, doing a data analysis comes with choices by the analyst. Do we include or exclude new build when analysing the difference profitability of two groups of brokers? Do we test a new campaign on all our users or just the serious ones? These choices influence the outcome of the analysis, and therefore are part of it. Always report these choices alongside the results, by including them in the report.  

Because of the subjective element in data analysis, it is always a good idea to have it reviewed before sharing the research. Just as code gets reviewed by a fresh pair of eyes before it is sent to production. Everyone doing an analysis hopes it will result in a find that will bring significant benefit to the business (and a little bit of fame for you). This might make you vulnerable for bad choices along the way that give more favourable results or ignoring uncertainty we have on an estimates, reporting it with more certainty then we should. (Yes, even data scientists are susceptable to this, we all want to come up with new things). Discussing your analysis with a critical other will safeguard you against turning hopes in false results. The reviewer should be just as responsible for the results as the analyst, so he / she cannot casually glance over the results. After all devoting time and other resources to fake findings is more harmful than concluding we are not convinced the effect is there. 

Speaking of these so-called null findings, they might not be as sexy as the discovery of a large, game-changing effect, but they contribute to collective knowledge as well. It is therefore essential to share these results in the same manner as you would do with found effects. They might bust long-existing myths about the business and often result in new research questions. Take pride in doing an analysis thoroughly and report the result whatever it is. 

Finally, to create a steady and trustworthy knowledge base, we should make sure that the analyses done are reproducible. We should always be able to trace exactly where the results are coming from, even a while after the analysis was done. The best way to create a reproducible result is by doing the analysis using a scripting language. Every step along the way is then saved in the script and the analysis is then reproduced by rerunning the script. However, not everyone has the time available that should be invested to learn a scripting language. Many analyses are done by point and click tools, such as Google Analytics or Excel. Reproducibility of important insights could then be assured by creating a README in which the steps taken are documented. How you do it is not important, but it is crucial to make your analysis reproducible. As a check ask yourself the question: If I left the company, would someone else a year from now be able to redo the analysis and arrive at the same results? If the answer for some reason is no, take all the necessary steps to make sure it is yes. 