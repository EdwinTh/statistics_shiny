4 Effect size 

Statistical tests answer the question based on the sample at hand, can we conclude there is an effect in the population? With an effect we usually mean a difference between groups, such as click-through-rates of different designs or number of purchases between two types of campaigns. There are two things that influence the confidence we have that an observed effect in the sample is real, thus not due to chance because of sampling. They are the number of sample cases, as discussed at two, and the size of the effect in the sample. If you find an effect on a large sample, it is less likely this due to chance than when your sample is small. And confidence grows as the effect in the sample gets larger. 

Note that a statistical test, such as the infamous p-value or the Bayesian posteriors we are using, has nothing to say about the actual size of the effect. It only reflects the confidence we have in the effect being real as a mathematical reflection of the sample and effect size. Many researchers stop at the question “is the effect real?” by passing the statistical test or not. However, you should always ask the follow-up question “is the effect large enough to be relevant?”. This question should be answered in light of the business decision that is tied to the outcome of the data analysis. The follow-up actions on the outcome of the analysis usually differ in costs. If the new design has a click-through-rate that is 0.09% higher than the old design, this effect may prove to be statistically real if it is tested on thousands on users. However, if it takes a team four sprints to implement, the benefits in click-through-rate might not outweigh the costs of implementation and we choose to stick to the old design. 

It is always the responsibility of the analyst to communicate the estimated effect size. It does not suffice to claim one is better, but you should always say how much better. 